{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a5a4a8",
   "metadata": {},
   "source": [
    "### Section bodies combined by score order top 1000 words\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Get the sections which have the scores greater than cutoff 0.744291\n",
    "2. Extract the body of sections\n",
    "3. Normalize the scores and adjusted number of words to be picked from the section\n",
    "4. Maximize the word allocation in case some sections doesn't have required word\n",
    "4. Extract top k words\n",
    "5. If no relevant sections -> Write the complete file as summary with top 1000 words\n",
    "6. Compare the system summary with each gold summary and get the rouge score. Take average of all gold summaries for the respective file.\n",
    "7. Take average of all the scores for all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1e8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from extract_section_body import extract_section_body\n",
    "from rouge_evaluation import get_rouge_scores\n",
    "from maximal_word_allocation import get_number_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86aed7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DATASET = True\n",
    "TEST_DATASET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed50bbcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../Dataset/FNS2023_Datasets/English/validation'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if VALIDATION_DATASET:\n",
    "    dir_ = '../../Dataset/FNS2023_Datasets/English/validation'\n",
    "    toc_loc_pkl_file_path = '../../Dataset/FNS2023_Datasets/English/validation/out/valid_toc_loc.pkl'\n",
    "    df_predicted_path = '../../DiMSum_FNP_2022/2_Section_Classification/out/validation_df_predicted.pkl'\n",
    "\n",
    "if TEST_DATASET:\n",
    "    dir_ = '../../../Dataset/FNS2022/English/testing/'\n",
    "    toc_loc_pkl_file_path = '../../../Dataset/Annotated_Dataset/test_toc_loc.pkl'\n",
    "    df_predicted_path = '../../FNP2022/2_Section_Classification/out/test_df_predicted.pkl'\n",
    "\n",
    "annual_reports_dir = \"annual_reports\"\n",
    "gold_summary_dir = \"gold_summaries\"\n",
    "system_summary_dir = 'GPT_Summaries'\n",
    "team_name = 'SSC_AI_RG'\n",
    "dir_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ae5bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>toc_section</th>\n",
       "      <th>toc_section_pos</th>\n",
       "      <th>toc_section_len</th>\n",
       "      <th>is_section_in_summary</th>\n",
       "      <th>toc_section_cleaned</th>\n",
       "      <th>pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30777</td>\n",
       "      <td>Financial and operational highlights</td>\n",
       "      <td>161</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>financi oper highlight</td>\n",
       "      <td>1</td>\n",
       "      <td>0.417846</td>\n",
       "      <td>0.582154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30777</td>\n",
       "      <td>Strategic report</td>\n",
       "      <td>183</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>strateg report</td>\n",
       "      <td>0</td>\n",
       "      <td>0.931694</td>\n",
       "      <td>0.068306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30777</td>\n",
       "      <td>Global network</td>\n",
       "      <td>189</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>global network</td>\n",
       "      <td>0</td>\n",
       "      <td>0.756982</td>\n",
       "      <td>0.243018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30777</td>\n",
       "      <td>Chairman’s statement</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>chairman statement</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018714</td>\n",
       "      <td>0.981286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30777</td>\n",
       "      <td>Chief Executive’s review</td>\n",
       "      <td>204</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>chief execut review</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007735</td>\n",
       "      <td>0.992265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10547</th>\n",
       "      <td>4162</td>\n",
       "      <td>to 110,</td>\n",
       "      <td>21914</td>\n",
       "      <td>1104</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.980373</td>\n",
       "      <td>0.019627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10548</th>\n",
       "      <td>4162</td>\n",
       "      <td>and 117</td>\n",
       "      <td>23018</td>\n",
       "      <td>1104</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.982939</td>\n",
       "      <td>0.017061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10549</th>\n",
       "      <td>4162</td>\n",
       "      <td>to 116</td>\n",
       "      <td>24122</td>\n",
       "      <td>1104</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.985174</td>\n",
       "      <td>0.014826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10550</th>\n",
       "      <td>4162</td>\n",
       "      <td>to 122</td>\n",
       "      <td>25226</td>\n",
       "      <td>5440</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.984885</td>\n",
       "      <td>0.015115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10551</th>\n",
       "      <td>4162</td>\n",
       "      <td>to 135</td>\n",
       "      <td>30666</td>\n",
       "      <td>5440</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.992461</td>\n",
       "      <td>0.007539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10552 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_id                           toc_section  toc_section_pos  \\\n",
       "0        30777  Financial and operational highlights              161   \n",
       "1        30777                      Strategic report              183   \n",
       "2        30777                        Global network              189   \n",
       "3        30777                  Chairman’s statement              200   \n",
       "4        30777              Chief Executive’s review              204   \n",
       "...        ...                                   ...              ...   \n",
       "10547     4162                               to 110,            21914   \n",
       "10548     4162                               and 117            23018   \n",
       "10549     4162                                to 116            24122   \n",
       "10550     4162                                to 122            25226   \n",
       "10551     4162                                to 135            30666   \n",
       "\n",
       "       toc_section_len  is_section_in_summary     toc_section_cleaned  pred  \\\n",
       "0                   22                      0  financi oper highlight     1   \n",
       "1                    6                      1          strateg report     0   \n",
       "2                   11                      0          global network     0   \n",
       "3                    4                      1      chairman statement     1   \n",
       "4                    4                      1     chief execut review     1   \n",
       "...                ...                    ...                     ...   ...   \n",
       "10547             1104                      0                             0   \n",
       "10548             1104                      0                             0   \n",
       "10549             1104                      0                             0   \n",
       "10550             5440                      0                             0   \n",
       "10551             5440                      0                             0   \n",
       "\n",
       "          False      True  \n",
       "0      0.417846  0.582154  \n",
       "1      0.931694  0.068306  \n",
       "2      0.756982  0.243018  \n",
       "3      0.018714  0.981286  \n",
       "4      0.007735  0.992265  \n",
       "...         ...       ...  \n",
       "10547  0.980373  0.019627  \n",
       "10548  0.982939  0.017061  \n",
       "10549  0.985174  0.014826  \n",
       "10550  0.984885  0.015115  \n",
       "10551  0.992461  0.007539  \n",
       "\n",
       "[10552 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicted = pickle.load(open(df_predicted_path, 'rb'))\n",
    "df_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32233535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_sections_with_score(file_id):\n",
    "    cutoff_score = 0.744291\n",
    "    df_dict = df_predicted[df_predicted.file_id == int(file_id)][['toc_section', 'True']].to_dict('list')\n",
    "    section_score_dict = {}\n",
    "    toc_sections = df_dict['toc_section']\n",
    "    section_scores = df_dict['True']\n",
    "    for i in range(len(toc_sections)):\n",
    "        if section_scores[i] >= cutoff_score:\n",
    "            section_score_dict[toc_sections[i]] = section_scores[i]\n",
    "    return section_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40b9eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_sections_with_body_len(file_id):\n",
    "    section_body_len_dict = {}\n",
    "    section_score_dict = get_relevant_sections_with_score(file_id)\n",
    "    for section in section_score_dict.keys():\n",
    "        body = extract_section_body(file_id, section, dir_, annual_reports_dir, toc_loc_pkl_file_path)\n",
    "        section_body_len_dict[section] = len(body.split(' '))\n",
    "    return section_body_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "795b782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_number_of_words(file_id):\n",
    "    section_num_words_dict = {}\n",
    "    section_score_dict = get_relevant_sections_with_score(file_id)\n",
    "    sections = list(section_score_dict.keys())\n",
    "    section_scores = np.array(list(section_score_dict.values()))\n",
    "    section_body_len_dict = get_relevant_sections_with_body_len(file_id)\n",
    "    section_body_len = np.array(list(section_body_len_dict.values()))\n",
    "    prev_num_required_words = np.zeros(len(section_body_len))\n",
    "    num_words = get_number_of_words(section_scores, section_body_len, 1000, prev_num_required_words)\n",
    "    for i in range(len(sections)):\n",
    "        section_num_words_dict[sections[i]] = int(num_words[i])\n",
    "    return section_num_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee92d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_file = 0\n",
    "# Create a directory for the system-generated summaries\n",
    "# os.makedirs(system_summary_dir)\n",
    "\n",
    "# Iterate through files in the annual_reports_dir\n",
    "for file in os.listdir(os.path.join(dir_, annual_reports_dir)):\n",
    "    try:\n",
    "        print(\"Processing File Number: \", num_file)\n",
    "        num_file = num_file + 1\n",
    "        \n",
    "        # Extract the file_id from the filename\n",
    "        file_id = file.split('.')[0]\n",
    "        \n",
    "        # Get relevant sections and their scores for the file\n",
    "        relevant_sections_with_score = get_relevant_sections_with_score(file_id)\n",
    "        \n",
    "        # Section order is maintained\n",
    "        relevant_sections = list(relevant_sections_with_score.keys())\n",
    "        \n",
    "        # Get the number of words in each section\n",
    "        section_num_words_dict = get_section_number_of_words(file_id)\n",
    "        \n",
    "        summary = \"\"\n",
    "        total_number_of_words_in_body = 0\n",
    "        total_number_of_words_in_summary = 0\n",
    "        \n",
    "        print(file_id, relevant_sections, section_num_words_dict)\n",
    "        \n",
    "        if relevant_sections:\n",
    "            print('Relevant Section Found in ', file_id)\n",
    "            \n",
    "            # Iterate through relevant sections\n",
    "            for section in relevant_sections:\n",
    "                number_of_words_to_be_extracted = section_num_words_dict[section]\n",
    "                \n",
    "                # Extract the body of the section\n",
    "                section_body = extract_section_body(file_id, section, dir_, annual_reports_dir, toc_loc_pkl_file_path)\n",
    "                section_body_split = section_body.split(' ')\n",
    "                print(section_body_split)\n",
    "                number_of_words_in_body = len(section_body_split)\n",
    "                total_number_of_words_in_body = total_number_of_words_in_body + number_of_words_in_body\n",
    "                \n",
    "                # Generate the summary based on word limits\n",
    "                if number_of_words_in_body > number_of_words_to_be_extracted:\n",
    "                    summary = summary + \" \".join(section_body_split[:number_of_words_to_be_extracted])\n",
    "                    total_number_of_words_in_summary = total_number_of_words_in_summary + number_of_words_to_be_extracted\n",
    "                else:\n",
    "                    print(file_id, section, number_of_words_in_body, number_of_words_to_be_extracted)\n",
    "                    summary = summary + \" \".join(section_body_split[:number_of_words_in_body])\n",
    "                    total_number_of_words_in_summary = total_number_of_words_in_summary + number_of_words_in_body\n",
    "            \n",
    "            print(file_id, 'number_of_words_in_output_summary', total_number_of_words_in_summary)    \n",
    "            print(file_id, 'number_of_words_in_body', total_number_of_words_in_body)\n",
    "            print('\\n')\n",
    "        else:\n",
    "            print('Relevant Section Not Found in ', file_id)\n",
    "            \n",
    "            # Read the entire summary if no relevant sections are found\n",
    "            summary = open(os.path.join(dir_, annual_reports_dir, file), \"r\", encoding=\"utf-8\").read()\n",
    "            summary_split = summary.split(' ')\n",
    "            number_of_words = len(summary_split)\n",
    "            \n",
    "            # Generate a shorter summary if the original summary is too long\n",
    "            if number_of_words > 1000:\n",
    "                summary = \" \".join(summary_split[:1000])\n",
    "        \n",
    "        # Write the generated summary to a text file\n",
    "        with open(os.path.join(system_summary_dir, file_id + '_' + team_name + '.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(str(summary))\n",
    "        \n",
    "        # Skip processing if the file is \".DS_Store\"\n",
    "        if \".DS_Store\" in file:\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(file, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VALIDATION_DATASET:\n",
    "    gold_summary_dir_ =  os.path.join(dir_, gold_summary_dir)\n",
    "    rouge_scores = get_rouge_scores(system_summary_dir, gold_summary_dir_)\n",
    "    rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d632c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63c670",
   "metadata": {},
   "source": [
    "#### Langchain Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35712a5",
   "metadata": {},
   "source": [
    "##### Short note\n",
    "- get_relevant_sections_with_body_len\n",
    "- get_section_number_of_words\n",
    "- get_relevant_sections_with_score\n",
    "- We are using all the aboove functions to retrieve the necessary parameters to send as input to GPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8db8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be7922f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='be51f10009fa41258fcd750a2fba07f2', openai_api_base='https://openai-ss.openai.azure.com/', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None, deployment_name='ss-gpt-32k', openai_api_type='azure', openai_api_version='2023-03-15-preview')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_GPT_3_5_TURBO = False\n",
    "USE_GPT_4 = True\n",
    "\n",
    "# Access - Config \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"be51f10009fa41258fcd750a2fba07f2\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://openai-ss.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-03-15-preview\"\n",
    "\n",
    "# Assign model \n",
    "if USE_GPT_4:\n",
    "    model = AzureChatOpenAI(temperature=0,deployment_name=\"ss-gpt-32k\")\n",
    "elif USE_GPT_3_5_TURBO:\n",
    "    model = AzureChatOpenAI(temperature=0,deployment_name=\"ss-gpt\")\n",
    "else:\n",
    "    raise Exception('Model not supported.')\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86f64757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated response schema\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"Summary\", description=\"summary of the section name from the section content around specified words\"),\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8328ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## zero shot prompt tempelate\n",
    "zero_shot_template = \"\"\"\n",
    "The task is to summarize the section \"{section_name}\" with the following content: \"{section_content}\"\n",
    "\n",
    "Instructions:\n",
    "- Create a summary of approximately {number_of_words} words.\n",
    "- Ensure that the output language matches the input language.\n",
    "- The summary should be a coherent and complete paragraph.\n",
    "\n",
    "Format Instructions:\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53fd5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt():\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "    \n",
    "    print('Zero shot setting')\n",
    "    prompt_template = zero_shot_template\n",
    "    input_vars = [\"section_name\", \"section_content\",\"number_of_words\"]\n",
    "        \n",
    "    prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "            HumanMessagePromptTemplate.from_template(prompt_template)\n",
    "    ],\n",
    "        input_variables=input_vars,\n",
    "        partial_variables={\"format_instructions\": format_instructions}\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6c8639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero shot setting\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7598621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary(section_name, section_content, number_of_words, prompt):\n",
    "    # Format the input variables into the prompt\n",
    "    _input = prompt.format_prompt(section_name=section_name, section_content=section_content, number_of_words=number_of_words)\n",
    "    \n",
    "    # Generate model output using the formatted input\n",
    "    output = model(_input.to_messages())\n",
    "    \n",
    "    # Parse the model's response using the output_parser\n",
    "    response = output_parser.parse(output.content)\n",
    "    \n",
    "    # Return the parsed response (summary)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7030e2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing File Number:  1\n",
      "Processing File Number:  2\n",
      "Processing File Number:  3\n",
      "Processing File Number:  4\n",
      "Processing File Number:  5\n",
      "Processing File Number:  6\n",
      "Processing File Number:  7\n",
      "Processing File Number:  8\n",
      "Processing File Number:  9\n",
      "Processing File Number:  10\n",
      "Processing File Number:  0\n",
      "Number of files processed:  10\n",
      "{'rouge-1': {'p': 0.5189054978341309, 'r': 0.3906467464124276, 'f': 0.36883269308185795}, 'rouge-2': {'p': 0.21004519574485517, 'r': 0.19747814859354768, 'f': 0.17067486118804331}}\n",
      "Processing File Number:  11\n",
      "Processing File Number:  12\n",
      "Processing File Number:  13\n",
      "Processing File Number:  14\n",
      "Processing File Number:  15\n",
      "Processing File Number:  16\n",
      "Processing File Number:  17\n",
      "Processing File Number:  18\n",
      "Processing File Number:  19\n",
      "Processing File Number:  20\n",
      "Processing File Number:  0\n",
      "Number of files processed:  20\n",
      "{'rouge-1': {'p': 0.5524000081554427, 'r': 0.3741936458788636, 'f': 0.3801297648677684}, 'rouge-2': {'p': 0.21592620452439087, 'r': 0.16571272310800328, 'f': 0.15882930856095673}}\n",
      "Processing File Number:  21\n",
      "Processing File Number:  22\n",
      "Processing File Number:  23\n",
      "Processing File Number:  24\n",
      "Processing File Number:  25\n",
      "Processing File Number:  26\n",
      "Processing File Number:  27\n",
      "Processing File Number:  28\n",
      "Processing File Number:  29\n",
      "Processing File Number:  30\n",
      "Processing File Number:  0\n",
      "Number of files processed:  30\n",
      "{'rouge-1': {'p': 0.5355864140096004, 'r': 0.38675470848680354, 'f': 0.3867727467046961}, 'rouge-2': {'p': 0.212733175871525, 'r': 0.16843517845588352, 'f': 0.1610159695882807}}\n",
      "Processing File Number:  31\n",
      "Processing File Number:  32\n",
      "Processing File Number:  33\n",
      "Processing File Number:  34\n",
      "Processing File Number:  35\n",
      "Processing File Number:  36\n",
      "Processing File Number:  37\n",
      "Processing File Number:  38\n",
      "Processing File Number:  39\n",
      "Processing File Number:  40\n",
      "Processing File Number:  0\n",
      "Number of files processed:  40\n",
      "{'rouge-1': {'p': 0.538581787788349, 'r': 0.37859348941129645, 'f': 0.3883399312833757}, 'rouge-2': {'p': 0.21840927611710184, 'r': 0.16140860958574224, 'f': 0.16077176805327892}}\n",
      "Processing File Number:  41\n",
      "Processing File Number:  42\n",
      "Processing File Number:  43\n",
      "Processing File Number:  44\n",
      "Processing File Number:  45\n",
      "Processing File Number:  46\n",
      "Processing File Number:  47\n",
      "Processing File Number:  48\n",
      "Processing File Number:  49\n",
      "Processing File Number:  50\n",
      "Processing File Number:  0\n",
      "Number of files processed:  50\n",
      "{'rouge-1': {'p': 0.5468358662679699, 'r': 0.3662742629855047, 'f': 0.38651838676888056}, 'rouge-2': {'p': 0.22198918510019738, 'r': 0.15357653861449005, 'f': 0.15840077425478571}}\n",
      "Processing File Number:  51\n",
      "Processing File Number:  52\n",
      "Processing File Number:  53\n",
      "Processing File Number:  54\n",
      "Processing File Number:  55\n",
      "Processing File Number:  56\n",
      "Processing File Number:  57\n",
      "Processing File Number:  58\n",
      "Processing File Number:  59\n",
      "Processing File Number:  60\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Number of files processed:  60\n",
      "{'rouge-1': {'p': 0.5567362461491372, 'r': 0.3801635002660037, 'f': 0.40224534219406005}, 'rouge-2': {'p': 0.23682113265750937, 'r': 0.1677398964027766, 'f': 0.17352069320120703}}\n",
      "Processing File Number:  61\n",
      "Processing File Number:  62\n",
      "Processing File Number:  63\n",
      "Processing File Number:  64\n",
      "Processing File Number:  65\n",
      "Processing File Number:  66\n",
      "Processing File Number:  67\n",
      "Processing File Number:  68\n",
      "Processing File Number:  69\n",
      "Processing File Number:  70\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Number of files processed:  70\n",
      "{'rouge-1': {'p': 0.556032825327876, 'r': 0.3791583531758947, 'f': 0.40317896285392274}, 'rouge-2': {'p': 0.2356643939086353, 'r': 0.16521400958811475, 'f': 0.17214564467269022}}\n",
      "Processing File Number:  71\n",
      "Processing File Number:  72\n",
      "Relevant Section Not Found in  31014\n",
      "Processing File Number:  73\n",
      "Processing File Number:  74\n",
      "Relevant Section Not Found in  31022\n",
      "Processing File Number:  75\n",
      "Processing File Number:  76\n",
      "Processing File Number:  77\n",
      "Processing File Number:  78\n",
      "Processing File Number:  79\n",
      "Processing File Number:  80\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Number of files processed:  80\n",
      "{'rouge-1': {'p': 0.5551892784991967, 'r': 0.3718142486866821, 'f': 0.3991055309654686}, 'rouge-2': {'p': 0.23973812605488284, 'r': 0.16292951034822734, 'f': 0.17216956452469856}}\n",
      "Processing File Number:  81\n",
      "Processing File Number:  82\n",
      "Processing File Number:  83\n",
      "Processing File Number:  84\n",
      "Processing File Number:  85\n",
      "Processing File Number:  86\n",
      "Processing File Number:  87\n",
      "Processing File Number:  88\n",
      "Processing File Number:  89\n",
      "Processing File Number:  90\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Number of files processed:  90\n",
      "{'rouge-1': {'p': 0.5597902414152587, 'r': 0.3711708728500897, 'f': 0.4020720997714623}, 'rouge-2': {'p': 0.24401512212532184, 'r': 0.16327981214679052, 'f': 0.17465795776224516}}\n",
      "Processing File Number:  91\n",
      "Processing File Number:  92\n",
      "Processing File Number:  93\n",
      "Processing File Number:  94\n",
      "Processing File Number:  95\n",
      "Processing File Number:  96\n",
      "Processing File Number:  97\n",
      "Processing File Number:  98\n",
      "Processing File Number:  99\n",
      "Processing File Number:  100\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Number of files processed:  100\n",
      "{'rouge-1': {'p': 0.5681826973037906, 'r': 0.3828256323743539, 'f': 0.41429250458187555}, 'rouge-2': {'p': 0.2545392912142125, 'r': 0.17317783143117257, 'f': 0.1855627828871055}}\n",
      "Processing File Number:  101\n",
      "Processing File Number:  102\n",
      "Processing File Number:  103\n",
      "Processing File Number:  104\n",
      "Processing File Number:  105\n",
      "Processing File Number:  106\n",
      "Processing File Number:  107\n",
      "Processing File Number:  108\n",
      "Processing File Number:  109\n",
      "Processing File Number:  110\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Processing File Number:  100\n",
      "Number of files processed:  110\n",
      "{'rouge-1': {'p': 0.5743501343991368, 'r': 0.3756866060933395, 'f': 0.4105709019653725}, 'rouge-2': {'p': 0.25562197188347335, 'r': 0.16872093660105228, 'f': 0.18220546681921454}}\n",
      "Processing File Number:  111\n",
      "Processing File Number:  112\n",
      "Processing File Number:  113\n",
      "Processing File Number:  114\n",
      "Processing File Number:  115\n",
      "Processing File Number:  116\n",
      "Processing File Number:  117\n",
      "Processing File Number:  118\n",
      "Processing File Number:  119\n",
      "Processing File Number:  120\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Processing File Number:  100\n",
      "Number of files processed:  120\n",
      "{'rouge-1': {'p': 0.5817468018890299, 'r': 0.37686082272798, 'f': 0.41502989638941634}, 'rouge-2': {'p': 0.260555608580327, 'r': 0.1705893523642827, 'f': 0.18581987046867585}}\n",
      "Processing File Number:  121\n",
      "Processing File Number:  122\n",
      "Processing File Number:  123\n",
      "Processing File Number:  124\n",
      "Processing File Number:  125\n",
      "Processing File Number:  126\n",
      "Processing File Number:  127\n",
      "Processing File Number:  128\n",
      "Processing File Number:  129\n",
      "Processing File Number:  130\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Processing File Number:  100\n",
      "Number of files processed:  130\n",
      "{'rouge-1': {'p': 0.5830708099534248, 'r': 0.3747934347840684, 'f': 0.4143202852418261}, 'rouge-2': {'p': 0.25998398494734776, 'r': 0.16812915758978444, 'f': 0.18408269854379442}}\n",
      "Processing File Number:  131\n",
      "Processing File Number:  132\n",
      "Processing File Number:  133\n",
      "Relevant Section Not Found in  31478\n",
      "Processing File Number:  134\n",
      "Processing File Number:  135\n",
      "Processing File Number:  136\n",
      "Processing File Number:  137\n",
      "Processing File Number:  138\n",
      "Processing File Number:  139\n",
      "Processing File Number:  140\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Processing File Number:  100\n",
      "Number of files processed:  140\n",
      "{'rouge-1': {'p': 0.5732934429311669, 'r': 0.37973217093508155, 'f': 0.41541824912068354}, 'rouge-2': {'p': 0.25337852456320714, 'r': 0.1676238222497691, 'f': 0.18218411082040914}}\n",
      "Processing File Number:  141\n",
      "Processing File Number:  142\n",
      "Processing File Number:  143\n",
      "Processing File Number:  144\n",
      "Processing File Number:  145\n",
      "Processing File Number:  146\n",
      "Processing File Number:  147\n",
      "Processing File Number:  148\n",
      "Processing File Number:  149\n",
      "Processing File Number:  150\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Processing File Number:  100\n",
      "Number of files processed:  150\n",
      "{'rouge-1': {'p': 0.5672100288074247, 'r': 0.38285240228778117, 'f': 0.41469845498129415}, 'rouge-2': {'p': 0.2493083859050042, 'r': 0.16793670029643296, 'f': 0.18068974303016322}}\n",
      "Processing File Number:  151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing File Number:  152\n",
      "Processing File Number:  153\n",
      "Processing File Number:  154\n",
      "Processing File Number:  155\n",
      "Processing File Number:  156\n",
      "Processing File Number:  157\n",
      "Processing File Number:  158\n",
      "Processing File Number:  159\n",
      "Processing File Number:  160\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Processing File Number:  100\n",
      "Processing File Number:  150\n",
      "Number of files processed:  160\n",
      "{'rouge-1': {'p': 0.5625626279424665, 'r': 0.3871832675776511, 'f': 0.414683003790807}, 'rouge-2': {'p': 0.24832743510604693, 'r': 0.17120949386719525, 'f': 0.18172678093074618}}\n",
      "Processing File Number:  161\n",
      "Processing File Number:  162\n",
      "Processing File Number:  163\n",
      "Processing File Number:  164\n",
      "Processing File Number:  165\n",
      "Processing File Number:  166\n",
      "Processing File Number:  167\n",
      "Processing File Number:  168\n",
      "Processing File Number:  169\n",
      "Processing File Number:  170\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Processing File Number:  100\n",
      "Processing File Number:  150\n",
      "Number of files processed:  170\n",
      "{'rouge-1': {'p': 0.5563980570308544, 'r': 0.3916558336297696, 'f': 0.4150319086276501}, 'rouge-2': {'p': 0.2459156302132003, 'r': 0.17200626854989753, 'f': 0.1819399530274381}}\n",
      "Processing File Number:  171\n",
      "Relevant Section Not Found in  31677\n",
      "Processing File Number:  172\n",
      "Processing File Number:  173\n",
      "Processing File Number:  174\n",
      "Processing File Number:  175\n",
      "Processing File Number:  176\n",
      "Processing File Number:  177\n",
      "Processing File Number:  178\n",
      "Processing File Number:  179\n",
      "Relevant Section Not Found in  31766\n",
      "Processing File Number:  180\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Processing File Number:  100\n",
      "Processing File Number:  150\n",
      "Number of files processed:  180\n",
      "{'rouge-1': {'p': 0.5535769781639737, 'r': 0.39979591919169927, 'f': 0.41821648977785425}, 'rouge-2': {'p': 0.24719893976635032, 'r': 0.17820064401796584, 'f': 0.1858654354148528}}\n",
      "Processing File Number:  181\n",
      "Processing File Number:  182\n",
      "Processing File Number:  183\n",
      "Processing File Number:  184\n",
      "Processing File Number:  185\n",
      "Processing File Number:  186\n",
      "Processing File Number:  187\n",
      "Processing File Number:  188\n",
      "Processing File Number:  189\n",
      "Processing File Number:  190\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Processing File Number:  100\n",
      "Processing File Number:  150\n",
      "Number of files processed:  190\n",
      "{'rouge-1': {'p': 0.5520727045052275, 'r': 0.4021618526887638, 'f': 0.41885823553158724}, 'rouge-2': {'p': 0.24665844819069999, 'r': 0.18005310692097795, 'f': 0.18675740142721942}}\n",
      "Processing File Number:  191\n",
      "Processing File Number:  192\n",
      "Processing File Number:  193\n",
      "Processing File Number:  194\n",
      "Processing File Number:  195\n",
      "Processing File Number:  196\n",
      "Processing File Number:  197\n",
      "Processing File Number:  198\n",
      "Processing File Number:  199\n",
      "Processing File Number:  200\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Processing File Number:  100\n",
      "Processing File Number:  150\n",
      "Number of files processed:  200\n",
      "{'rouge-1': {'p': 0.5472007017698872, 'r': 0.40326595094269246, 'f': 0.4173300097696622}, 'rouge-2': {'p': 0.2425909784227122, 'r': 0.17925256636392323, 'f': 0.18488542235283936}}\n",
      "Processing File Number:  201\n",
      "Processing File Number:  202\n",
      "Processing File Number:  203\n",
      "Processing File Number:  204\n",
      "Processing File Number:  205\n",
      "Processing File Number:  206\n",
      "Processing File Number:  207\n",
      "Processing File Number:  208\n",
      "Processing File Number:  209\n",
      "Processing File Number:  210\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Processing File Number:  100\n",
      "Processing File Number:  150\n",
      "Processing File Number:  200\n",
      "Number of files processed:  210\n",
      "{'rouge-1': {'p': 0.5474649770909733, 'r': 0.40066737340051306, 'f': 0.41529553093890637}, 'rouge-2': {'p': 0.24355486284802147, 'r': 0.1780835538982642, 'f': 0.1840531356377091}}\n",
      "Processing File Number:  211\n",
      "Processing File Number:  212\n",
      "Processing File Number:  213\n",
      "Processing File Number:  214\n",
      "Processing File Number:  215\n",
      "Processing File Number:  216\n",
      "Processing File Number:  217\n",
      "Processing File Number:  218\n",
      "Processing File Number:  219\n",
      "Processing File Number:  220\n",
      "Processing File Number:  0\n",
      "Processing File Number:  50\n",
      "Processing File Number:  100\n",
      "Processing File Number:  150\n",
      "Processing File Number:  200\n",
      "Number of files processed:  220\n",
      "{'rouge-1': {'p': 0.5495527107501337, 'r': 0.4056558824985663, 'f': 0.4192243143422334}, 'rouge-2': {'p': 0.24796028845776116, 'r': 0.18289651363398154, 'f': 0.18856845677021428}}\n",
      "Processing File Number:  221\n",
      "Processing File Number:  222\n",
      "Processing File Number:  223\n",
      "Processing File Number:  224\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty list to store the rows\n",
    "summary_data = []\n",
    "rouge_score_list = []\n",
    "os.makedirs(system_summary_dir)\n",
    "num_files_to_process = 300  # Change this to the desired number of files to process\n",
    "\n",
    "num_file = 0\n",
    "\n",
    "# Iterate through files in the annual_reports_dir\n",
    "for file in os.listdir(os.path.join(dir_, annual_reports_dir)):\n",
    "    try:\n",
    "        if num_file >= num_files_to_process:\n",
    "            break\n",
    "            \n",
    "        num_file = num_file + 1\n",
    "        \n",
    "        \n",
    "        print(\"Processing File Number: \", num_file)\n",
    "        \n",
    "        \n",
    "        # Extract the file_id and file_name from the filename\n",
    "        file_id = file.split('.')[0]\n",
    "        file_name = file\n",
    "        \n",
    "        # Get relevant sections and their scores for the file\n",
    "        relevant_sections_with_score = get_relevant_sections_with_score(file_id)\n",
    "        \n",
    "        # Section order is maintained\n",
    "        relevant_sections = list(relevant_sections_with_score.keys())\n",
    "        \n",
    "        # Get the number of words in each section\n",
    "        section_num_words_dict = get_section_number_of_words(file_id)\n",
    "        \n",
    "        summary = \"\"\n",
    "        \n",
    "        if relevant_sections:\n",
    "    \n",
    "            # Iterate through relevant sections\n",
    "            for section in relevant_sections:\n",
    "                \n",
    "                number_of_words_to_be_extracted = section_num_words_dict[section]\n",
    "                section_body = extract_section_body(file_id, section, dir_, annual_reports_dir, toc_loc_pkl_file_path)\n",
    "                \n",
    "                summary_resp = extract_summary(section, section_body, number_of_words_to_be_extracted, prompt)\n",
    "                summary = summary + summary_resp['Summary']\n",
    "                \n",
    "                # Create a dictionary for the row and add it to the list\n",
    "                row = {\n",
    "                    'file_id': file_id,\n",
    "                    'file_name': file_name,\n",
    "                    'narrative_section_name': section,\n",
    "                    'narrative_section_body': section_body,\n",
    "                    'num_words_to_be_extracted': number_of_words_to_be_extracted,\n",
    "                    'summary_generated': summary_resp['Summary']\n",
    "                }\n",
    "                summary_data.append(row)\n",
    "            \n",
    "        else:\n",
    "            print('Relevant Section Not Found in ', file_id)\n",
    "            \n",
    "            # Read the entire summary if no relevant sections are found\n",
    "            summary = open(os.path.join(dir_, annual_reports_dir, file), \"r\", encoding=\"utf-8\").read()\n",
    "            summary_split = summary.split(' ')\n",
    "            number_of_words = len(summary_split)\n",
    "            \n",
    "            # Generate a shorter summary if the original summary is too long\n",
    "            if number_of_words > 1000:\n",
    "                summary = \" \".join(summary_split[:1000])\n",
    "            \n",
    "            # Create a dictionary for the row and add it to the list\n",
    "            row = {\n",
    "                'file_id': file_id,\n",
    "                'file_name': file_name,\n",
    "                'narrative_section_name': '',\n",
    "                'narrative_section_body': '',\n",
    "                'num_words_to_be_extracted': '',\n",
    "                'summary_generated': summary\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "        \n",
    "        with open(os.path.join(system_summary_dir, file_id + '_' + team_name + '.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(str(summary))\n",
    "            if relevant_sections:\n",
    "                row = {\n",
    "                    'file_id': file_id,\n",
    "                    'file_name': file_name,\n",
    "                    'narrative_section_name': 'Final Summary',\n",
    "                    'narrative_section_body': '',\n",
    "                    'num_words_to_be_extracted': '',\n",
    "                    'summary_generated': summary\n",
    "                }\n",
    "                summary_data.append(row)\n",
    "            \n",
    "        # Skip processing if the file is \".DS_Store\"\n",
    "        if \".DS_Store\" in file:\n",
    "            continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(file, e)\n",
    "        \n",
    "    if num_file % 10 == 0 : \n",
    "            if VALIDATION_DATASET:\n",
    "                gold_summary_dir_ =  os.path.join(dir_, gold_summary_dir)\n",
    "                rouge_scores = get_rouge_scores(system_summary_dir, gold_summary_dir_)\n",
    "                print(rouge_scores)\n",
    "                rouge_score_list.append(rouge_scores)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VALIDATION_DATASET:\n",
    "    gold_summary_dir_ =  os.path.join(dir_, gold_summary_dir)\n",
    "    rouge_scores = get_rouge_scores(system_summary_dir, gold_summary_dir_)\n",
    "    print(rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9373cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_csv('result.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
