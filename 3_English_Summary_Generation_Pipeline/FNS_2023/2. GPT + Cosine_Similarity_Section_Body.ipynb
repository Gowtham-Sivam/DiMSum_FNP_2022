{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a5a4a8",
   "metadata": {},
   "source": [
    "### Section bodies combined by score order top 1000 words\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Get the sections which have the scores greater than cutoff 0.744291\n",
    "2. Extract the body of sections\n",
    "3. Normalize the scores and adjusted number of words to be picked from the section\n",
    "4. Maximize the word allocation in case some sections doesn't have required word\n",
    "4. Extract top k words\n",
    "5. If no relevant sections -> Write the complete file as summary with top 1000 words\n",
    "6. Compare the system summary with each gold summary and get the rouge score. Take average of all gold summaries for the respective file.\n",
    "7. Take average of all the scores for all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1e8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")  # Adds the previous directory to the Python path\n",
    "\n",
    "\n",
    "from extract_section_body import extract_section_body\n",
    "from rouge_evaluation import get_rouge_scores\n",
    "from maximal_word_allocation import get_number_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86aed7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_DATASET = True\n",
    "TEST_DATASET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed50bbcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../Dataset/FNS2023_Datasets/English/validation/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if VALIDATION_DATASET:\n",
    "    dir_ = '../../Dataset/FNS2023_Datasets/English/validation/'\n",
    "    toc_loc_pkl_file_path = '../../Dataset/FNS2023_Datasets/English/validation/out/valid_toc_loc.pkl'\n",
    "    df_predicted_path = '../../../DiMSum_FNP_2022/2_Section_Classification/out/validation_df_predicted.pkl'\n",
    "\n",
    "if TEST_DATASET:\n",
    "    dir_ = '../../../Dataset/FNS2022/English/testing/'\n",
    "    toc_loc_pkl_file_path = '../../../Dataset/Annotated_Dataset/test_toc_loc.pkl'\n",
    "    df_predicted_path = '../../FNP2022/2_Section_Classification/out/test_df_predicted.pkl'\n",
    "\n",
    "annual_reports_dir = \"annual_reports\"\n",
    "gold_summary_dir = \"gold_summaries\"\n",
    "system_summary_dir = 'GPT_Summaries'\n",
    "team_name = 'SSC_AI_RG'\n",
    "dir_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ae5bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>toc_section</th>\n",
       "      <th>toc_section_pos</th>\n",
       "      <th>toc_section_len</th>\n",
       "      <th>is_section_in_summary</th>\n",
       "      <th>toc_section_cleaned</th>\n",
       "      <th>pred</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30777</td>\n",
       "      <td>Financial and operational highlights</td>\n",
       "      <td>161</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>financi oper highlight</td>\n",
       "      <td>1</td>\n",
       "      <td>0.412114</td>\n",
       "      <td>0.587886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30777</td>\n",
       "      <td>Strategic report</td>\n",
       "      <td>183</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>strateg report</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932721</td>\n",
       "      <td>0.067279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30777</td>\n",
       "      <td>Global network</td>\n",
       "      <td>189</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>global network</td>\n",
       "      <td>0</td>\n",
       "      <td>0.748624</td>\n",
       "      <td>0.251376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30777</td>\n",
       "      <td>Chairman’s statement</td>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>chairman statement</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018934</td>\n",
       "      <td>0.981066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30777</td>\n",
       "      <td>Chief Executive’s review</td>\n",
       "      <td>204</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>chief execut review</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006269</td>\n",
       "      <td>0.993731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9592</th>\n",
       "      <td>33155</td>\n",
       "      <td>Audit Committee report</td>\n",
       "      <td>6247</td>\n",
       "      <td>345</td>\n",
       "      <td>0</td>\n",
       "      <td>audit committe report</td>\n",
       "      <td>0</td>\n",
       "      <td>0.975820</td>\n",
       "      <td>0.024180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9593</th>\n",
       "      <td>33155</td>\n",
       "      <td>Directors’ Remuneration report</td>\n",
       "      <td>6592</td>\n",
       "      <td>1406</td>\n",
       "      <td>0</td>\n",
       "      <td>director remuner report</td>\n",
       "      <td>0</td>\n",
       "      <td>0.998092</td>\n",
       "      <td>0.001908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9594</th>\n",
       "      <td>33155</td>\n",
       "      <td>Corporate Governance Code</td>\n",
       "      <td>7998</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>corpor govern code</td>\n",
       "      <td>0</td>\n",
       "      <td>0.986258</td>\n",
       "      <td>0.013742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9595</th>\n",
       "      <td>33155</td>\n",
       "      <td>Directors’ biographies</td>\n",
       "      <td>8067</td>\n",
       "      <td>570</td>\n",
       "      <td>0</td>\n",
       "      <td>director biographi</td>\n",
       "      <td>0</td>\n",
       "      <td>0.991681</td>\n",
       "      <td>0.008319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9596</th>\n",
       "      <td>33155</td>\n",
       "      <td>Directors’ report</td>\n",
       "      <td>8637</td>\n",
       "      <td>7083</td>\n",
       "      <td>0</td>\n",
       "      <td>director report</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999256</td>\n",
       "      <td>0.000744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9597 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      file_id                           toc_section  toc_section_pos  \\\n",
       "0       30777  Financial and operational highlights              161   \n",
       "1       30777                      Strategic report              183   \n",
       "2       30777                        Global network              189   \n",
       "3       30777                  Chairman’s statement              200   \n",
       "4       30777              Chief Executive’s review              204   \n",
       "...       ...                                   ...              ...   \n",
       "9592    33155                Audit Committee report             6247   \n",
       "9593    33155        Directors’ Remuneration report             6592   \n",
       "9594    33155             Corporate Governance Code             7998   \n",
       "9595    33155                Directors’ biographies             8067   \n",
       "9596    33155                     Directors’ report             8637   \n",
       "\n",
       "      toc_section_len  is_section_in_summary      toc_section_cleaned  pred  \\\n",
       "0                  22                      0   financi oper highlight     1   \n",
       "1                   6                      1           strateg report     0   \n",
       "2                  11                      0           global network     0   \n",
       "3                   4                      1       chairman statement     1   \n",
       "4                   4                      1      chief execut review     1   \n",
       "...               ...                    ...                      ...   ...   \n",
       "9592              345                      0    audit committe report     0   \n",
       "9593             1406                      0  director remuner report     0   \n",
       "9594               69                      1       corpor govern code     0   \n",
       "9595              570                      0       director biographi     0   \n",
       "9596             7083                      0          director report     0   \n",
       "\n",
       "         False      True  \n",
       "0     0.412114  0.587886  \n",
       "1     0.932721  0.067279  \n",
       "2     0.748624  0.251376  \n",
       "3     0.018934  0.981066  \n",
       "4     0.006269  0.993731  \n",
       "...        ...       ...  \n",
       "9592  0.975820  0.024180  \n",
       "9593  0.998092  0.001908  \n",
       "9594  0.986258  0.013742  \n",
       "9595  0.991681  0.008319  \n",
       "9596  0.999256  0.000744  \n",
       "\n",
       "[9597 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicted = pickle.load(open(df_predicted_path, 'rb'))\n",
    "df_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32233535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_sections_with_score(file_id):\n",
    "    cutoff_score = 0.744291\n",
    "    df_dict = df_predicted[df_predicted.file_id == int(file_id)][['toc_section', 'True']].to_dict('list')\n",
    "    section_score_dict = {}\n",
    "    toc_sections = df_dict['toc_section']\n",
    "    section_scores = df_dict['True']\n",
    "    for i in range(len(toc_sections)):\n",
    "        if section_scores[i] >= cutoff_score:\n",
    "            section_score_dict[toc_sections[i]] = section_scores[i]\n",
    "    return section_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40b9eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_sections_with_body_len(file_id):\n",
    "    section_body_len_dict = {}\n",
    "    section_score_dict = get_relevant_sections_with_score(file_id)\n",
    "    for section in section_score_dict.keys():\n",
    "        body = extract_section_body(file_id, section, dir_, annual_reports_dir, toc_loc_pkl_file_path)\n",
    "        section_body_len_dict[section] = len(body.split(' '))\n",
    "    return section_body_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "795b782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_number_of_words(file_id):\n",
    "    section_num_words_dict = {}\n",
    "    section_score_dict = get_relevant_sections_with_score(file_id)\n",
    "    sections = list(section_score_dict.keys())\n",
    "    section_scores = np.array(list(section_score_dict.values()))\n",
    "    section_body_len_dict = get_relevant_sections_with_body_len(file_id)\n",
    "    section_body_len = np.array(list(section_body_len_dict.values()))\n",
    "    prev_num_required_words = np.zeros(len(section_body_len))\n",
    "    num_words = get_number_of_words(section_scores, section_body_len, 1000, prev_num_required_words)\n",
    "    for i in range(len(sections)):\n",
    "        section_num_words_dict[sections[i]] = int(num_words[i])\n",
    "    return section_num_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee92d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_file = 0\n",
    "# Create a directory for the system-generated summaries\n",
    "# os.makedirs(system_summary_dir)\n",
    "\n",
    "# Iterate through files in the annual_reports_dir\n",
    "for file in os.listdir(os.path.join(dir_, annual_reports_dir)):\n",
    "    try:\n",
    "        print(\"Processing File Number: \", num_file)\n",
    "        num_file = num_file + 1\n",
    "        \n",
    "        # Extract the file_id from the filename\n",
    "        file_id = file.split('.')[0]\n",
    "        \n",
    "        # Get relevant sections and their scores for the file\n",
    "        relevant_sections_with_score = get_relevant_sections_with_score(file_id)\n",
    "        \n",
    "        # Section order is maintained\n",
    "        relevant_sections = list(relevant_sections_with_score.keys())\n",
    "        \n",
    "        # Get the number of words in each section\n",
    "        section_num_words_dict = get_section_number_of_words(file_id)\n",
    "        \n",
    "        summary = \"\"\n",
    "        total_number_of_words_in_body = 0\n",
    "        total_number_of_words_in_summary = 0\n",
    "        \n",
    "        print(file_id, relevant_sections, section_num_words_dict)\n",
    "        \n",
    "        if relevant_sections:\n",
    "            print('Relevant Section Found in ', file_id)\n",
    "            \n",
    "            # Iterate through relevant sections\n",
    "            for section in relevant_sections:\n",
    "                number_of_words_to_be_extracted = section_num_words_dict[section]\n",
    "                \n",
    "                # Extract the body of the section\n",
    "                section_body = extract_section_body(file_id, section, dir_, annual_reports_dir, toc_loc_pkl_file_path)\n",
    "                section_body_split = section_body.split(' ')\n",
    "                print(section_body_split)\n",
    "                number_of_words_in_body = len(section_body_split)\n",
    "                total_number_of_words_in_body = total_number_of_words_in_body + number_of_words_in_body\n",
    "                \n",
    "                # Generate the summary based on word limits\n",
    "                if number_of_words_in_body > number_of_words_to_be_extracted:\n",
    "                    summary = summary + \" \".join(section_body_split[:number_of_words_to_be_extracted])\n",
    "                    total_number_of_words_in_summary = total_number_of_words_in_summary + number_of_words_to_be_extracted\n",
    "                else:\n",
    "                    print(file_id, section, number_of_words_in_body, number_of_words_to_be_extracted)\n",
    "                    summary = summary + \" \".join(section_body_split[:number_of_words_in_body])\n",
    "                    total_number_of_words_in_summary = total_number_of_words_in_summary + number_of_words_in_body\n",
    "            \n",
    "            print(file_id, 'number_of_words_in_output_summary', total_number_of_words_in_summary)    \n",
    "            print(file_id, 'number_of_words_in_body', total_number_of_words_in_body)\n",
    "            print('\\n')\n",
    "        else:\n",
    "            print('Relevant Section Not Found in ', file_id)\n",
    "            \n",
    "            # Read the entire summary if no relevant sections are found\n",
    "            summary = open(os.path.join(dir_, annual_reports_dir, file), \"r\", encoding=\"utf-8\").read()\n",
    "            summary_split = summary.split(' ')\n",
    "            number_of_words = len(summary_split)\n",
    "            \n",
    "            # Generate a shorter summary if the original summary is too long\n",
    "            if number_of_words > 1000:\n",
    "                summary = \" \".join(summary_split[:1000])\n",
    "        \n",
    "        # Write the generated summary to a text file\n",
    "        with open(os.path.join(system_summary_dir, file_id + '_' + team_name + '.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(str(summary))\n",
    "        \n",
    "        # Skip processing if the file is \".DS_Store\"\n",
    "        if \".DS_Store\" in file:\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(file, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VALIDATION_DATASET:\n",
    "    gold_summary_dir_ =  os.path.join(dir_, gold_summary_dir)\n",
    "    rouge_scores = get_rouge_scores(system_summary_dir, gold_summary_dir_)\n",
    "    rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d632c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63c670",
   "metadata": {},
   "source": [
    "#### Langchain Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35712a5",
   "metadata": {},
   "source": [
    "##### Short note\n",
    "- get_relevant_sections_with_body_len\n",
    "- get_section_number_of_words\n",
    "- get_relevant_sections_with_score\n",
    "- We are using all the aboove functions to retrieve the necessary parameters to send as input to GPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e417729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from sentence_transformers) (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.16.4)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: numpy in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.22.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.31.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.7.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.6.3)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2022.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (2.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.3.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\gowtham\\anaconda3\\lib\\site-packages (from torchvision->sentence_transformers) (9.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8db8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be7922f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='be51f10009fa41258fcd750a2fba07f2', openai_api_base='https://openai-ss.openai.azure.com/', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None, deployment_name='ss-gpt-32k', openai_api_type='azure', openai_api_version='2023-03-15-preview')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_GPT_3_5_TURBO = False\n",
    "USE_GPT_4 = True\n",
    "\n",
    "# Access - Config \n",
    "\n",
    "\n",
    "# Assign model \n",
    "if USE_GPT_4:\n",
    "    model = AzureChatOpenAI(temperature=0,deployment_name=\"ss-gpt-32k\")\n",
    "elif USE_GPT_3_5_TURBO:\n",
    "    model = AzureChatOpenAI(temperature=0,deployment_name=\"ss-gpt\")\n",
    "else:\n",
    "    raise Exception('Model not supported.')\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86f64757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated response schema\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"Summary\", description=\"summary of the section name from the section content around specified words\"),\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8328ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## zero shot prompt tempelate\n",
    "zero_shot_template = \"\"\"\n",
    "The task is to summarize the section \"{section_name}\" with the following content: \"{section_content}\"\n",
    "\n",
    "Instructions:\n",
    "- Create a summary of approximately {number_of_words} words.\n",
    "- Ensure that the output language matches the input language.\n",
    "- The summary should be a coherent and complete paragraph.\n",
    "\n",
    "Format Instructions:\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53fd5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt():\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "    \n",
    "    print('Zero shot setting')\n",
    "    prompt_template = zero_shot_template\n",
    "    input_vars = [\"section_name\", \"section_content\",\"number_of_words\"]\n",
    "        \n",
    "    prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "            HumanMessagePromptTemplate.from_template(prompt_template)\n",
    "    ],\n",
    "        input_variables=input_vars,\n",
    "        partial_variables={\"format_instructions\": format_instructions}\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6c8639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero shot setting\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7598621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary(section_name, section_content, number_of_words, prompt):\n",
    "    # Format the input variables into the prompt\n",
    "    _input = prompt.format_prompt(section_name=section_name, section_content=section_content, number_of_words=number_of_words)\n",
    "    \n",
    "    # Generate model output using the formatted input\n",
    "    output = model(_input.to_messages())\n",
    "    \n",
    "    # Parse the model's response using the output_parser\n",
    "    response = output_parser.parse(output.content)\n",
    "    \n",
    "    # Return the parsed response (summary)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aa82a190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' During the year the Chairman met with \\nshareholders to discuss governance matters', '\\nThe Chairman’s statement, the Chief Executive’s review, the Chief Financial Officer’s report and all the information contained on pages 36  \\nto 75 together comprise the Directors’ report for the year ended 31 December 2017 ', ' Quinn \\nCBE\\nChairman \\nChairman’s statement\\n“One of the key roles of the Chairman is to ensure that the Board \\nmembers possess a range of complementary skills which are relevant to \\nBodycote’s business']\n"
     ]
    }
   ],
   "source": [
    "def section_relevant_section_body(file_id, section_title, dir_path, reports_dir):\n",
    "    \n",
    "    # Load the content of the report file\n",
    "    with open(os.path.join(dir_path, reports_dir, str(file_id) + '.txt'), \"r\", encoding='utf-8') as report_file:\n",
    "        txt_file_content = report_file.read()\n",
    "    \n",
    "    # Load the Sentence Transformer model\n",
    "    model_name = \"all-MiniLM-L6-v2\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Tokenize the txt_file_content into sentences ( change the sentence splitting logic )\n",
    "    sentences = txt_file_content.split(\".\")  \n",
    "\n",
    "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    section_title_embedding = model.encode(section_title, convert_to_tensor=True)\n",
    "\n",
    "    # Calculate cosine similarities between section_title and sentence embeddings\n",
    "    cosine_similarities = util.pytorch_cos_sim(section_title_embedding, sentence_embeddings)[0]\n",
    "\n",
    "    # Sorting sentences\n",
    "    sorted_indices = cosine_similarities.argsort(descending=True)\n",
    "\n",
    "    top_n = 3  \n",
    "    relevant_section_data = [sentences[i] for i in sorted_indices[:top_n]]\n",
    "\n",
    "    return relevant_section_data\n",
    "    \n",
    "relevant_sections_body = section_relevant_section_body('30777', 'Chairman Statement', dir_ , annual_reports_dir)\n",
    "print(relevant_sections_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7030e2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty list to store the rows\n",
    "summary_data = []\n",
    "rouge_score_list = []\n",
    "os.makedirs(system_summary_dir)\n",
    "num_files_to_process = 300  # Change this to the desired number of files to process\n",
    "\n",
    "num_file = 0\n",
    "\n",
    "# Iterate through files in the annual_reports_dir\n",
    "for file in os.listdir(os.path.join(dir_, annual_reports_dir)):\n",
    "    try:\n",
    "        if num_file >= num_files_to_process:\n",
    "            break\n",
    "            \n",
    "        num_file = num_file + 1\n",
    "        \n",
    "        \n",
    "        print(\"Processing File Number: \", num_file)\n",
    "        \n",
    "        \n",
    "        # Extract the file_id and file_name from the filename\n",
    "        file_id = file.split('.')[0]\n",
    "        file_name = file\n",
    "        \n",
    "        # Get relevant sections and their scores for the file\n",
    "        relevant_sections_with_score = get_relevant_sections_with_score(file_id)\n",
    "        \n",
    "        # Section order is maintained\n",
    "        relevant_sections = list(relevant_sections_with_score.keys())\n",
    "        \n",
    "        # Get the number of words in each section\n",
    "        section_num_words_dict = get_section_number_of_words(file_id)\n",
    "        \n",
    "        summary = \"\"\n",
    "        \n",
    "        if relevant_sections:\n",
    "    \n",
    "            # Iterate through relevant sections\n",
    "            for section in relevant_sections:\n",
    "                \n",
    "                number_of_words_to_be_extracted = section_num_words_dict[section]\n",
    "                section_body = section_relevant_section_body(file_id, section, dir_, annual_reports_dir)\n",
    "                \n",
    "                summary_resp = extract_summary(section, section_body, number_of_words_to_be_extracted, prompt)\n",
    "                summary = summary + summary_resp['Summary']\n",
    "                \n",
    "                # Create a dictionary for the row and add it to the list\n",
    "                row = {\n",
    "                    'file_id': file_id,\n",
    "                    'file_name': file_name,\n",
    "                    'narrative_section_name': section,\n",
    "                    'narrative_section_body': section_body,\n",
    "                    'num_words_to_be_extracted': number_of_words_to_be_extracted,\n",
    "                    'summary_generated': summary_resp['Summary']\n",
    "                }\n",
    "                summary_data.append(row)\n",
    "            \n",
    "        else:\n",
    "            print('Relevant Section Not Found in ', file_id)\n",
    "            \n",
    "            # Read the entire summary if no relevant sections are found\n",
    "            summary = open(os.path.join(dir_, annual_reports_dir, file), \"r\", encoding=\"utf-8\").read()\n",
    "            summary_split = summary.split(' ')\n",
    "            number_of_words = len(summary_split)\n",
    "            \n",
    "            # Generate a shorter summary if the original summary is too long\n",
    "            if number_of_words > 1000:\n",
    "                summary = \" \".join(summary_split[:1000])\n",
    "            \n",
    "            \n",
    "            row = {\n",
    "                'file_id': file_id,\n",
    "                'file_name': file_name,\n",
    "                'narrative_section_name': '',\n",
    "                'narrative_section_body': '',\n",
    "                'num_words_to_be_extracted': '',\n",
    "                'summary_generated': summary\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "        \n",
    "        with open(os.path.join(system_summary_dir, file_id + '_' + team_name + '.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(str(summary))\n",
    "            if relevant_sections:\n",
    "                row = {\n",
    "                    'file_id': file_id,\n",
    "                    'file_name': file_name,\n",
    "                    'narrative_section_name': 'Final Summary',\n",
    "                    'narrative_section_body': '',\n",
    "                    'num_words_to_be_extracted': '',\n",
    "                    'summary_generated': summary\n",
    "                }\n",
    "                summary_data.append(row)\n",
    "            \n",
    "        # Skip processing if the file is \".DS_Store\"\n",
    "        if \".DS_Store\" in file:\n",
    "            continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(file, e)\n",
    "        \n",
    "    if num_file % 10 == 0 : \n",
    "            if VALIDATION_DATASET:\n",
    "                gold_summary_dir_ =  os.path.join(dir_, gold_summary_dir)\n",
    "                rouge_scores = get_rouge_scores(system_summary_dir, gold_summary_dir_)\n",
    "                print(rouge_scores)\n",
    "                rouge_score_list.append(rouge_scores)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VALIDATION_DATASET:\n",
    "    gold_summary_dir_ =  os.path.join(dir_, gold_summary_dir)\n",
    "    rouge_scores = get_rouge_scores(system_summary_dir, gold_summary_dir_)\n",
    "    print(rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9373cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_csv('result.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
